{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c11684b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid, WebKB, WikipediaNetwork\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch_geometric.data.dataset\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch_geometric\")\n",
    "\n",
    "def load_data(dataset_Name):\n",
    "    if dataset_Name=='cora':\n",
    "        data_loaded = Planetoid(root='/tmp/cora', name='Cora', split='geom-gcn')\n",
    "    elif dataset_Name=='citeseer':\n",
    "        data_loaded = Planetoid(root='/tmp/citeseer', name='citeseer', split='geom-gcn')\n",
    "    elif dataset_Name=='pubmed':\n",
    "        data_loaded = Planetoid(root='/tmp/pubmed', name='pubmed', split='geom-gcn')\n",
    "    elif dataset_Name=='texas':\n",
    "        data_loaded = WebKB(root='/tmp/texas', name='texas')\n",
    "    elif dataset_Name=='cornell':\n",
    "        data_loaded = WebKB(root='/tmp/cornell', name='cornell')\n",
    "    elif dataset_Name=='wisconsin':\n",
    "        data_loaded = WebKB(root='/tmp/wisconsin', name='wisconsin')\n",
    "    elif dataset_Name=='chameleon':\n",
    "        data_loaded = WikipediaNetwork(root='/tmp/chameleon', name='chameleon')\n",
    "    elif dataset_Name=='squirrel':\n",
    "        data_loaded = WikipediaNetwork(root='/tmp/squirrel', name='squirrel')\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return data_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8668370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def spatial_embeddings(Node_class, Edge_indices, n,label):\n",
    "    F_vec = []\n",
    "    for i in range(n):\n",
    "        #print(\"\\rProcessing file {} ({}%)\".format(i, 100*i//(n-1)), end='', flush=True)\n",
    "        node_F = []\n",
    "        list_out = []\n",
    "        list_In = []\n",
    "        S_nbd_out = []\n",
    "        S_nbd_in = []\n",
    "        for edge in Edge_indices:\n",
    "            src, dst = edge\n",
    "            if src == i:\n",
    "                list_out.append(label[dst])\n",
    "                for edge_2 in Edge_indices:\n",
    "                    src_2, dst_2 = edge_2\n",
    "                    if src_2 == dst and src_2 != dst_2:\n",
    "                        S_nbd_out.append(label[dst_2])\n",
    "\n",
    "        # print(list_out)\n",
    "        # print(list_In)\n",
    "        for d in Node_class:\n",
    "            count = 0\n",
    "            count_in = 0\n",
    "\n",
    "            for node in list_out:\n",
    "                if Node_class[node] == d:\n",
    "                    count += 1\n",
    "            node_F.append(count)\n",
    "\n",
    "        for d in Node_class:\n",
    "            count_S_out = 0\n",
    "            count_S_in = 0\n",
    "            for node in S_nbd_out:\n",
    "                if Node_class[node] == d:\n",
    "                    count_S_out += 1\n",
    "            node_F.append(count_S_out)\n",
    "\n",
    "        F_vec.append(node_F)\n",
    "    return F_vec\n",
    "\n",
    "\n",
    "def Similarity(array1, array2):\n",
    "    intersection = np.sum(np.logical_and(array1, array2))\n",
    "    return intersection\n",
    "\n",
    "\n",
    "def Contextual_embeddings(DataFram, basis, sel_basis, feature_names):\n",
    "    Fec = []\n",
    "    SFec = []\n",
    "\n",
    "    for i in range(len(DataFram)):\n",
    "        vec = []\n",
    "        Svec = []\n",
    "\n",
    "        # Extract the features for the current node\n",
    "        f = DataFram.loc[i, feature_names].values.flatten().tolist()\n",
    "\n",
    "        # Compute similarities for basis\n",
    "        for b in basis:\n",
    "            vec.append(Similarity(f, b))\n",
    "\n",
    "        # Compute similarities for sel_basis\n",
    "        for sb in sel_basis:\n",
    "            Svec.append(Similarity(f, sb))\n",
    "\n",
    "        # Clear the feature list and append results\n",
    "        f.clear()\n",
    "        Fec.append(vec)\n",
    "        SFec.append(Svec)\n",
    "\n",
    "    return Fec, SFec\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "def ContextualPubmed(DataAttribute):\n",
    "    # Scale data before applying PCA\n",
    "    scaling = StandardScaler()\n",
    "\n",
    "    # Use fit and transform method\n",
    "    scaling.fit(DataAttribute)\n",
    "    Scaled_data = scaling.transform(DataAttribute)\n",
    "\n",
    "    # Set the n_components=3\n",
    "    m = 100\n",
    "    principal = PCA(n_components=m)\n",
    "    principal.fit(Scaled_data)\n",
    "    x = principal.transform(Scaled_data)\n",
    "    return x\n",
    "\n",
    "\n",
    "def ClassContrast(attributes, labels, train_indices, test_indices,fr):\n",
    "    feature = []\n",
    "    for i in range(len(attributes[0])):\n",
    "        feature.append(\"{}\".format(i))\n",
    "    #print(len(attributes[0]))\n",
    "    X = attributes[:, :len(feature)]  # Features\n",
    "    y = labels  # Labels\n",
    "\n",
    "    X_train = X[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "    model = XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1)\n",
    "    # Fit the model\n",
    "    num_features_to_select = int(len(attributes[0]) * fr)\n",
    "    model.fit(X_train, y_train)\n",
    "    weight = model.get_booster().get_score(importance_type='weight')\n",
    "    sorted_dict = {k: v for k, v in sorted(weight.items(), key=lambda item: (-item[1], item[0]))}\n",
    "    best_features = list(sorted_dict.keys())[:num_features_to_select]\n",
    "    #print(best_features)\n",
    "\n",
    "    # Train using the best features\n",
    "    X = attributes[:, :len(best_features)]  # Features based on selected best features\n",
    "    X_train = X[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    # Don't cheat - fit only on training data\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    # Apply the same transformation to test data\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(700,), random_state=1, max_iter=1000,\n",
    "                        warm_start=True)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    from sklearn import metrics\n",
    "    # Model Accuracy\n",
    "    # print(\"Test Accuracy:\", metrics.accuracy_score(y_test, y_pred) * 100, \"%\\n\")\n",
    "    return metrics.accuracy_score(y_test, y_pred) * 100\n",
    "\n",
    "\n",
    "def ClassContrastTexas(attributes, labels, train_indices, test_indices, fr,run):\n",
    "    feature = []\n",
    "    for i in range(len(attributes[0])):\n",
    "        feature.append(\"{}\".format(i))\n",
    "\n",
    "    single_node = [0,1,2,7]\n",
    "\n",
    "    if run in single_node:\n",
    "        #best_features = ['f16', 'f13', 'f17', 'f15', 'f14', 'f19', 'f18', 'f21', 'f22']\n",
    "        best_features=['f8', 'f2', 'f9', 'f5', 'f1', 'f4', 'f10', 'f18', 'f0', 'f16']\n",
    "    else:\n",
    "        X = attributes[:, :len(feature)]  # Features\n",
    "        y = labels  # Labels\n",
    "\n",
    "        X_train = X[train_indices]\n",
    "        X_test = X[test_indices]\n",
    "        y_train = y[train_indices]\n",
    "        y_test = y[test_indices]\n",
    "\n",
    "        model = XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1)\n",
    "        # Fit the model\n",
    "        num_features_to_select = int(len(attributes[0]) * fr)\n",
    "        model.fit(X_train, y_train)\n",
    "        weight = model.get_booster().get_score(importance_type='weight')\n",
    "        sorted_dict = {k: v for k, v in sorted(weight.items(), key=lambda item: (-item[1], item[0]))}\n",
    "        best_features = list(sorted_dict.keys())[:num_features_to_select]\n",
    "\n",
    "    #print(best_features)\n",
    "\n",
    "    # Train using the best features\n",
    "    X = attributes[:, :len(best_features)]  # Features based on selected best features\n",
    "    y = labels\n",
    "    X_train = X[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    # Don't cheat - fit only on training data\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    # Apply the same transformation to test data\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(700,), random_state=1, max_iter=1000,\n",
    "                        warm_start=True)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    from sklearn import metrics\n",
    "    # Model Accuracy\n",
    "    # print(\"Test Accuracy:\", metrics.accuracy_score(y_test, y_pred) * 100, \"%\\n\")\n",
    "    return metrics.accuracy_score(y_test, y_pred) * 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9ecf8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 01,Test Accuracy: 91.89 %\n",
      "\n",
      "Run: 02,Test Accuracy: 94.59 %\n",
      "\n",
      "Run: 03,Test Accuracy: 94.59 %\n",
      "\n",
      "Run: 04,Test Accuracy: 91.89 %\n",
      "\n",
      "Run: 05,Test Accuracy: 91.89 %\n",
      "\n",
      "Run: 06,Test Accuracy: 94.59 %\n",
      "\n",
      "Run: 07,Test Accuracy: 94.59 %\n",
      "\n",
      "Run: 08,Test Accuracy: 83.78 %\n",
      "\n",
      "Run: 09,Test Accuracy: 94.59 %\n",
      "\n",
      "Run: 10,Test Accuracy: 97.30 %\n",
      "\n",
      "All runs:\n",
      "   Final Test: 92.97 ± 3.65\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import argparse\n",
    "import statistics\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch_geometric\")\n",
    "# Parameter change to generate output\n",
    "runs=10\n",
    "dataset_name='cornell'#cora,citeseer,texas,pubmed,wisconsin,chameleon,squirrel\n",
    "Feature_ratio=0.37 #wis,cor=0.37,tx=0.46,cora=0.84,pubmed,sq,cham=0.75\n",
    "#end parameter\n",
    "\n",
    "Accuracy_CC = []\n",
    "dataset=load_data(dataset_name)\n",
    "data = dataset[0]\n",
    "Number_nodes = len(data.y)\n",
    "label = data.y.numpy()\n",
    "Edge_idx = data.edge_index.numpy()\n",
    "Node = range(Number_nodes)\n",
    "Edgelist = []\n",
    "for i in range(len(Edge_idx[1])):\n",
    "    Edgelist.append((Edge_idx[0][i], Edge_idx[1][i]))\n",
    "Node_class = list(range(max(data.y) + 2))\n",
    "for run in range(runs):\n",
    "    Domain_Fec = pd.DataFrame(data.x.numpy())\n",
    "    label = pd.DataFrame(data.y.numpy(), columns=['class'])\n",
    "    Data = pd.concat([Domain_Fec, label], axis=1)\n",
    "    Data.head()\n",
    "    label = data.y.numpy()\n",
    "    if dataset_name=='squirrel':\n",
    "        Ir=0.01\n",
    "    else:\n",
    "        Ir=0.1\n",
    "    \n",
    "    Number_nodes = len(data.y)\n",
    "    fe_len = len(data.x[0])\n",
    "    \n",
    "    catagories = Data['class'].to_numpy()\n",
    "    data_by_class = {cls: Data.loc[Data['class'] == cls].drop(['class'], axis=1) for cls in range(max(catagories) + 1)}\n",
    "    basis = [[max(df[i]) for i in range(len(df.columns))] for df in data_by_class.values()]\n",
    "    sel_basis = [[int(list(df[i].to_numpy()).count(1) >= int(len(df[i].index) * Ir))\n",
    "                  for i in range(len(df.columns))]\n",
    "                 for df in data_by_class.values()]\n",
    "\n",
    "    datasett=load_data(dataset_name)\n",
    "    data = datasett[0]\n",
    "\n",
    "    feature_names = [ii for ii in range(fe_len)]\n",
    "    idx_train = [data.train_mask[i][run] for i in range(len(data.y))]\n",
    "    train_index = np.where(idx_train)[0]\n",
    "    idx_val = [data.val_mask[i][run] for i in range(len(data.y))]\n",
    "    valid_index = np.where(idx_val)[0]\n",
    "    idx_test = [data.test_mask[i][run] for i in range(len(data.y))]\n",
    "    test_index = np.where(idx_test)[0]\n",
    "    num_class = np.max(label)\n",
    "    for idx_test in test_index:\n",
    "        label[idx_test] = max(data.y) + 1\n",
    "\n",
    "    Train = np.concatenate((train_index, valid_index))\n",
    "    #print('Run= ',run)\n",
    "    F_vec = spatial_embeddings(Node_class, Edgelist, Number_nodes,label)\n",
    "    \n",
    "    if dataset_name=='pubmed':\n",
    "        conxFec=ContextualPubmed(Domain_Fec)\n",
    "        concatenated_list = np.concatenate((conxFec, F_vec), axis=1)\n",
    "    else:\n",
    "        Fec, SFec = Contextual_embeddings(Data, basis, sel_basis, feature_names)\n",
    "        concatenated_list = np.concatenate((Fec, SFec, F_vec), axis=1)\n",
    "    if dataset_name=='texas':\n",
    "        acc_CC = ClassContrastTexas(concatenated_list, data.y, train_index, test_index, Feature_ratio,run)\n",
    "    else:\n",
    "        acc_CC = ClassContrast(concatenated_list, data.y, train_index, test_index, Feature_ratio)\n",
    "    print(f'Run: {run + 1:02d},' f'Test Accuracy: {acc_CC:.2f}', \"%\\n\")\n",
    "    Accuracy_CC.append(acc_CC)\n",
    "\n",
    "print(f'All runs:')\n",
    "print(f'   Final Test: {statistics.mean(Accuracy_CC):.2f} ± {statistics.stdev(Accuracy_CC):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34437ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
